\section{Soft Norm}
\label{sec:soft_norms}

We define the \emph{soft} norm of a vector $\mf{x}$ as
\begin{equation*}
    \Vert\mf{x}\Vert_s = \sqrt{\Vert\mf{x}\Vert^2+\varepsilon_x^2}-\varepsilon_x
    \label{eq:soft_norm_gradient},
\end{equation*}
where $\varepsilon_x>0$ has units of $\mf{x}$. Notice that $\Vert\mf{0}\Vert_s=
0$.

The gradient of the soft norm is
\begin{equation*}
    \frac{\partial\Vert\mf{x}\Vert_s}{\partial \mf{x}} = \frac{\mf{x}}{\Vert\mf{x}\Vert_s+\varepsilon_x} = \hat{\mf{x}}_s,
\end{equation*}
where we defined the \emph{soft unit vector} $\hat{\mf{x}}_s$.

The Hessian of the soft norm, the gradient of $\hat{\mf{x}}_s$, is
\begin{eqnarray*}
    \frac{\partial\hat{\mf{x}}_s}{\partial\mf{x}}=\frac{\partial^2\Vert\mf{x}\Vert_s}{\partial\mf{x}^2}&=&\frac{1}{\Vert\mf{x}\Vert_s+\varepsilon_x}\left(\mf{I}-\mf{P}(\hat{\mf{x}}_s)\right)\nonumber\\
    &=&\frac{\mf{P}^\perp(\hat{\mf{x}}_s)}{{\Vert\mf{x}\Vert_s+\varepsilon_x}}
\end{eqnarray*}
where the projection matrix is defined as
$\mf{P}(\hat{\mf{v}})=\hat{\mf{v}}\otimes\hat{\mf{v}}$. Note that
$\mf{P}(\hat{\mf{v}}) \succeq 0$ and $\mf{P}^\perp(\hat{\mf{v}}) \succeq 0$ for
all unit vectors $\hat{\mf{v}}\in\mathbb{R}^n$. Therefore, the soft norm is twice
differentiable with positive semi-definite Hessian, and thus it is convex.

These expressions for the gradient and Hessian of the norm of a vector are still
valid in the limit $\varepsilon_x\rightarrow 0$,  but they are not well-defined
at  $\mf{x}=\mf{0}$. However, the \emph{soft} versions have the nice
property that they are numerically well-behaved near and at $\mf{x}=\mf{0}$, and they are
continuously differentiable.

